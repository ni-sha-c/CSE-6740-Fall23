\documentclass[final]{beamer}
\usepackage{amsmath,amssymb,amsthm,amsfonts,graphicx}
\usepackage{eulervm,verbatim}          
\usepackage[scaled]{helvet}
\usepackage[most]{tcolorbox}
\setbeamercolor{frametitle}{fg=black,bg=white} % Colors of the block titles
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\definecolor{darkcerulean}{rgb}{0.03, 0.27, 0.49}
\newcommand{\citesmall}[1]{[{\color{darkcerulean}\begin{small} \textbf{#1} \end{small}}]}
\setbeamertemplate{footline}[frame number]
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{graphicx}  % Required for including images
\usepackage{bbm}
\usepackage{booktabs} % Top and bottom rules for tables
\definecolor{burgundy}{rgb}{0.5, 0.0, 0.13}
\newcommand{\highlight}[1]{{\color{burgundy} \textbf{#1}}}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
    pdftitle={CSE6740-Lecture 20},
    pdfauthor={Nisha Chandramoorthy},
    pdflang={en-US}
}



%----------------------------------------------------------------------------------------
%	TITLE SECTION 
%----------------------------------------------------------------------------------------
\title{\begin{huge}{Lecture 19: Kernel PCA, Random projections, tSNE, Isomaps}\end{huge}} % Poster title


\author{Nisha Chandramoorthy} % Author(s)


%----------------------------------------------------------------------------------------

\begin{document}

\frame{\titlepage}

%----------------------------------------------------------------------------------------
%	OBJECTIVES
%----------------------------------------------------------------------------------------
\begin{frame}{Last time: PCA}
	\begin{itemize}
	\item when $E$ and $D$ are linear $\to$ PCA.
	\pause 
	\item $E(x) = Wx$, $D(z) = W^\top z$.
	\pause
\item Let $C = \sum_{i=1}^m x_i x_i^\top = X^\top X$ be the data correlation matrix, neglecting the $1/m$ factor.
	\pause 
	\item $C$ is symmetric and positive semi-definite, $C = V \Lambda V^\top$.
	\item Theorem PCA: among linear hypothesis classes, $E^* = V^\top$, $D^* = V,$ where $V$ is the matrix of eigenvectors of $C = X^\top X$.
	\end{itemize}
\end{frame}
\begin{frame}{Linear algebra review: Rayleigh Quotient}
\begin{itemize}
	\item For a square matrix $A \in \mathbb{R}^{d\times d},$ the Rayleigh quotient is a scalar function,
	\begin{align*}
		r(x) = \frac{x^\top A x}{x^\top x}.
	\end{align*}
	\pause
	\item Eigenvalues of $A$ are the stationary points of $r(x)$.
	\pause
	\item $\nabla r(x) = \frac{2}{x^\top x} (Ax - r(x)x)$.
\end{itemize}
\end{frame}
\begin{frame}{PCA by SVD}
	\begin{itemize}
		\item When $m > d,$ do eigenvalue decomposition of $X^\top X$ or SVD of $X$.
		\pause
		\item When $m < d,$ do eigenvalue decomposition of $XX^\top.$ If $v_1, v_2,\cdots, v_n$ are the $n$ largest eigenvectors, principal vectors are $\frac{1}{\|X^\top v_i\|} X^\top v_i$.
		\pause
		\item Computational complexity: $O(\min(m^2d, md^2))$.

	\end{itemize}

\end{frame}
\begin{frame}{PCA properties}
	\begin{itemize}
		\item Exact when $X^\top X$ is rank $n.$
		\pause
	\item Maximizes variance. Let $x$ be a random vector chosen uniformly from centered data $x_1,\cdots,x_m.$ Then, for any $w \in \mathbb{R}^d,$ 
		$${\rm var}(x\cdot w) = \dfrac{1}{m}\sum_{i=1}^m (w\cdot x_i)^2.$$
	\pause
\item First principal component maximizes ${\rm var}(x\cdot w)$ over all $w$ with $\|w\| = 1.$ (See Ex 23.4 in book.)
	\pause
	\item Informally, PCA rotates the data so that the variance is maximized along the first axis, then the second, and so on.
	\pause
	\item Separates dissimilar points
			
	\end{itemize}
\end{frame}
\begin{frame}{Kernel PCA}
	\begin{itemize}
		\item Let $V$ be the matrix of the top $n$ eigenvectors of $K = X X^\top \in \mathbb{R}^{m\times m}.$ 
		\pause
		\item Then, principal vectors are $d^*_i = \frac{1}{\|X^\top v_i\|} X^\top v_i,$ $i = 1,2,\cdots,n.$
		\pause
		\item For some PD kernel, if $K_{ij} = k(x_i,x_j) = \langle \phi(x_i), \phi(x_j)\rangle = (X X^\top)_{ij},$ can compute $K$ only using kernel evaluations.
		
	\end{itemize}
\end{frame}
\begin{frame}{Graphical reprensentation of $X$}
	\begin{itemize}
		\item Choose weighting, such as, $w_{ij} = \exp(-\|x_i - x_j\|^2/2\sigma^2).$ As $\sigma \to 0,$ $w_{ij} \to \mathbbm{1}_{i=j}.$ The $m \times m$ matrix $W$ is the adjacency matrix of a graph.
		\pause
		\item Let $D$ be the diagonal matrix with $D_{ii} = \sum_{j=1}^m w_{ij}.$
		\pause 
		\item Graph laplacian: $L = D - W.$
		\pause 
		\item Detects local structure / clusters in data.

	\end{itemize}

\end{frame}
\begin{frame}{Laplacian eigenmaps}
	\begin{itemize}
		\item Want to solve: $\min_{y_1,\cdots,y_m} \sum_{i=1}^m \sum_{j=1}^m w_{ij} \|y_i - y_j\|^2.$
		\pause
		\item optimal embeddings: $y_i = E(x_i) = U[i,-n:]$ where $U$ is the matrix of eigenvectors of $L$.
		\pause
		\item For any vector $v$, $v^\top L v = (1/2)\sum_{i, j=1}^m w_{ij} (v_i - v_j)^2.$
		\pause
		\item $L$ is positive semi-definite.
	\end{itemize}
\end{frame}
\begin{frame}{Bottom $n$ eigenvectors}
	\begin{itemize}
		\item Rayleigh quotient optimality
		\pause
		\item Another interpretation: top $n$ eigenvectors of $L^\dagger.$ $L^\dagger_{ij}$ represents expected time for random walk $i \to j \to i.$ 
		\pause
		\item Kernel PCA with $K = L^\dagger$ is equivalent to Laplacian eigenmaps. 
	\end{itemize}
\end{frame}
\begin{frame}{Stochastic neighbor embedding [Hinton and Roweis 2002]}
\begin{itemize}
	\item Stochastic neighbor embedding(SNE): conditional probability that $x_i$ would pick $x_j$ as its neighbor, given by
	$$ p(x_j|x_i) = \frac{\exp(-\|x_i - x_j\|^2/2\sigma_i^2)}{\sum_{k\neq i} \exp(-\|x_i - x_k\|^2/2\sigma_i^2)}.$$
\pause
\item For the embeddings $y_i = E(x_i)$, 
	$$q(y_j|y_i) = \frac{\exp(-\|y_i - y_j\|^2)}{\sum_{k\neq i} \exp(-\|y_i - y_k\|^2)}.$$
\pause
\item SNE minimizes $\sum_{i=1}^m D_{\rm KL}(p_i||q_i)$, where $p_i$ and $q_i$ are the conditional probabilities of $x_i$ and $y_i$ respectively.
\pause
\item Penalizes large distances between $x_i$ and $x_j$ but also preserves local structure.
\end{itemize}
\end{frame}
\begin{frame}{tSNE [Van der Maaten and Hinton 2008]}
	\begin{itemize}
		\item tSNE cost function is $D_{\rm KL}(p||q) = \sum_{i=1}^m \sum_{j\neq i} p_{ij} \log \frac{p_{ij}}{q_{ij}},$ where $p_ij$ and $q_{ij}$ are the joint probabilities of $(x_i,x_j)$ and $(y_i,y_j)$ respectively.
		\item Changes joint distribution to a heavy-tailed distribution, $q(y_j, y_i) = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k\neq i} (1 + \|y_i - y_k\|^2)^{-1}}.$ 
		\pause
		\item approaches inverse square law on embedded space.

	\end{itemize}
\end{frame}
\begin{frame}{tSNE visualization}
\begin{figure}
	\includegraphics[width=0.49\textwidth]{tsne.png}
	\includegraphics[width=0.49\textwidth]{lle.png}
	\caption{From Van der Maaten and Hinton 2008. tSNE (left) and LLE (right) on MNIST dataset. }
\end{figure}
\end{frame}
\begin{frame}{Convolutional Neural Networks (source: cs231n.stanford.edu)}
	\begin{itemize}
		\item Suitable for image recognition. Won the 2012 ImageNet competition and subsequent ones.
		\item Three types of layers: convolutional, FC, pooling
		\item Convolutional layer: accepts a volume of size $W_1 \times H_1 \times D_1$ and outputs a volume of size $W_2 \times H_2 \times D_2$ where $W_2 = (W_1 - F + 2P)/S + 1$ and $H_2 = (H_1 - F + 2P)/S + 1$ and $D_2 = K$.

		\item $K$ is number of filters, $F$ is filter size, $S$ is stride, $P$ is padding.
		\item Pooling layer: downsamples along width and height, and optionally along depth.
		\item FC layer: computes class scores, resulting in volume of size $1 \times 1 \times K$.

	
	\end{itemize}
\end{frame}
\end{document}
